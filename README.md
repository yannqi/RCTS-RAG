# RCTS-RAG: Re-ranking Reasoning Context with Tree Search Makes Large Vision-Language Models Stronger

[![arXiv](https://img.shields.io/badge/arXiv-2506.07785-b31b1b.svg)](https://arxiv.org/pdf/2506.07785)
[![ICML 2025](https://img.shields.io/badge/ICML%202025-Spotlight-red)](https://icml.cc/)
[![Python](https://img.shields.io/badge/Python-3.8+-blue)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.13+-orange)](https://pytorch.org/)

**Authors**: [Qi Yang](https://yannqi.github.io/), Chenghao Zhang, Lubin Fan, Kun Ding, Jieping Ye, [Shiming Xiang](https://people.ucas.ac.cn/~xiangshiming)

This repository provides the official PyTorch implementation for **RCTS-RAG**, a novel approach that enhances Large Vision-Language Models (LVLMs) through re-ranking reasoning contexts using Monte Carlo Tree Search (MCTS). Our method has been accepted as a **Spotlight Paper** at ICML 2025.

## ğŸªµ TODO List

- [X] âœ… Release core implementation
- [X] âœ… Complete README documentation
- [X] âœ… Add configuration examples
- [ ] ğŸ”„ Add More detailed Quick Start.

## ğŸ”¥ What's New

- **(2025.5.1)** ğŸ‰ Our paper (RCTS) is accepted as **ICML 2025 Spotlight Paper**!
- **(2025.5.6)** ğŸ“„ Paper released on arXiv
- **(2025.8.31)** ğŸš€ Released the complete implementation code

## ğŸ“– Abstract
![1756659815270](image/README/1756659815270.png)
RCTS-RAG introduces a novel framework that combines Retrieval-Augmented Generation (RAG) with Monte Carlo Tree Search to improve the reasoning capabilities of Large Vision-Language Models. Our approach:

- ğŸ¯ **Re-ranks reasoning contexts** using MCTS to find optimal reasoning paths
- ğŸ§  **Enhances multi-modal understanding** by integrating visual and textual information
- ğŸ“Š **Achieves state-of-the-art performance** on multiple vision-language benchmarks
- ğŸ”„ **Supports multiple query modes** including hybrid, text-only, and random retrieval

## ğŸ—ï¸ Architecture

![1756659798225](image/README/1756659798225.png)

![1756659876968](image/README/1756659876968.png)

Our RCTS-RAG framework consists of three main components:

1. **Multi-modal Retrieval System**: Retrieves relevant contexts using hybrid text-image embeddings
2. **MCTS Re-ranking Module**: Explores and evaluates different reasoning paths using tree search
3. **Enhanced LVLMs**: Generates final answers based on re-ranked contexts

## ğŸ“Š Supported Datasets

RCTS-RAG supports evaluation on five major vision-language benchmarks:

- ğŸ§® **MathVista (MathV)**: Mathematical reasoning with visual elements
- ğŸ“ **ScienceQA**: Science question answering with diagrams
- ğŸ”¬ **MMMU**: Massive Multi-discipline Multimodal Understanding
- ğŸ‘ï¸ **VizWiz**: Visual question answering for the visually impaired
- ğŸ¯ **VSR (Visual Spatial Reasoning)**: Spatial reasoning tasks

## ğŸ› ï¸ Installation

### Prerequisites

- Python 3.8+
- PyTorch 1.13+
- CUDA (recommended for GPU acceleration)

### Setup Environment

```bash
# Clone the repository
git clone https://github.com/yannqi/RCTS-RAG.git
cd RCTS-RAG

# Create conda environment
conda create -n rcts-rag python=3.8
conda activate rcts-rag

# Install dependencies
pip install -r requirements.txt
```

### Key Dependencies

- `torch>=1.13.0`: Deep learning framework
- `transformers>=4.30.0`: Hugging Face transformers
- `faiss-cpu>=1.7.4`: Vector similarity search
- `flmr>=0.1.0`: Fine-grained Late-interaction Multi-modal Retriever
- `openai>=1.0.0`: OpenAI API support
- `omegaconf>=2.3.0`: Configuration management

## ğŸš€ Quick Start

### 1. Data Preparation

Configure your dataset paths in `configs/dataset_path.yaml`:

```yaml
dataset_path:
  ScienceQA: "/path/to/ScienceQA"
  MathV: "/path/to/MathVista"
  MMMU: "/path/to/MMMU"
  VizWiz: "/path/to/VizWiz"
  VSR_MC: "/path/to/VSR"
```

### 2. Index Construction

Build vector indices for retrieval:

```bash
# Example for ScienceQA
bash scripts/Index_Construct/ScienceQA_index.sh
```

### 3. Chain-of-Thought (CoT) Construction

Generate reasoning chains for MCTS:

```bash
# Example for ScienceQA
bash scripts/CoT_Construct/ScienceQA_CoT.sh
```

### 4. Run Evaluation

Execute RCTS-RAG on your chosen dataset:

```bash
# With MCTS re-ranking
bash scripts/RAG/ScienceQA_RAG_mcts_query.sh

# With hybrid query (no MCTS)
bash scripts/RAG/ScienceQA_RAG_hybrid_query.sh

# Without RAG (baseline)
bash scripts/RAG/ScienceQA_woRAG.sh
```

## ğŸ“ Project Structure

```
RCTS-RAG/
â”œâ”€â”€ configs/                    # Configuration files
â”‚   â”œâ”€â”€ main/                  # Main experiment configs
â”‚   â”œâ”€â”€ CoT_Pred/              # CoT prediction configs
â”‚   â”œâ”€â”€ extract_info/          # Information extraction configs
â”‚   â””â”€â”€ index_save/            # Index construction configs
â”œâ”€â”€ data/                      # Dataset implementations
â”‚   â”œâ”€â”€ ScienceQA.py          # ScienceQA dataset loader
â”‚   â”œâ”€â”€ MathV.py              # MathVista dataset loader
â”‚   â”œâ”€â”€ MMMU.py               # MMMU dataset loader
â”‚   â”œâ”€â”€ VizWiz.py             # VizWiz dataset loader
â”‚   â””â”€â”€ VSR_MC.py             # VSR dataset loader
â”œâ”€â”€ module/                    # Core modules
â”‚   â”œâ”€â”€ RAG/                  # RAG implementation
â”‚   â”‚   â”œâ”€â”€ RCTS_RAG.py       # Main RAG class
â”‚   â”‚   â””â”€â”€ answer.py         # Answer generation
â”‚   â”œâ”€â”€ mcts/                 # MCTS implementation
â”‚   â”‚   â”œâ”€â”€ mcts_llm.py       # MCTS with LLM
â”‚   â”‚   â””â”€â”€ mcts_reranking.py # Re-ranking logic
â”‚   â”œâ”€â”€ model/                # Model implementations
â”‚   â”‚   â”œâ”€â”€ llm.py            # LLM interfaces
â”‚   â”‚   â”œâ”€â”€ embedding_model.py # Embedding models
â”‚   â”‚   â””â”€â”€ query.py          # Query strategies
â”‚   â””â”€â”€ storage/              # Storage utilities
â”œâ”€â”€ scripts/                   # Execution scripts
â”‚   â”œâ”€â”€ RAG/                  # RAG experiment scripts
â”‚   â”œâ”€â”€ CoT_Construct/        # CoT construction scripts
â”‚   â”œâ”€â”€ Index_Construct/      # Index building scripts
â”‚   â””â”€â”€ evaluate/             # Evaluation scripts
â””â”€â”€ tools/                    # Utility tools
    â”œâ”€â”€ CoT_extract.py        # CoT extraction
    â”œâ”€â”€ extract_img_feats.py  # Image feature extraction
    â””â”€â”€ Index_construct.py    # Index construction
```

## âš™ï¸ Configuration

RCTS-RAG uses YAML configuration files for flexible experimentation. Key configuration categories:

### Main Experiment Config

Located in `configs/main/`, controls:

- Dataset selection
- Query modes (hybrid, text, MCTS)
- RAG parameters
- Model settings

### Query Modes

- **Hybrid Query**: Combines text and image embeddings
- **Text Query**: Text-only retrieval
- **MCTS Query**: Uses Monte Carlo Tree Search for re-ranking
- **Random Query**: Baseline random retrieval

### Example Configuration

```yaml
# configs/main/ScienceQA_mctsquery.yaml
LOG_DIR: "outputs/ScienceQA_mcts"
DATASET_NAME: "ScienceQA"
USE_RAG: true
USE_MCTS: true
TOP_K: 3
MCTS_TOP_K: 5
MCTS_ROLLOUTS: 10
QUERY_MODE: "hybrid_query"
INDEX_TYPE: "Hybrid_PreFLMR"
```

## ğŸ“ˆ Evaluation

### Run Single Dataset

```bash
# Evaluate on ScienceQA with MCTS
python main_baseline.py configs/main/ScienceQA_mctsquery.yaml

# Evaluate on MathVista without RAG
python main_baseline.py configs/main/MathV_woRAG.yaml
```

### Batch Evaluation

```bash
# Run all ScienceQA experiments
bash scripts/evaluate/eval_all_scienceqa.sh
```

## ğŸ¯ Key Features

### ğŸ”„ Monte Carlo Tree Search

- **Exploration**: Systematically explores different reasoning paths
- **Evaluation**: Uses reward models to assess path quality
- **Selection**: Chooses optimal reasoning contexts

### ğŸ” Multi-modal Retrieval

- **Hybrid Embeddings**: Combines text and image representations
- **FLMR Integration**: Uses Fine-grained Late-interaction Multi-modal Retriever
- **Efficient Indexing**: FAISS-based vector storage for fast retrieval

### ğŸ§  Enhanced Reasoning

- **Context Re-ranking**: Orders retrieved contexts by relevance
- **Chain-of-Thought**: Leverages step-by-step reasoning
- **Multi-turn Interaction**: Supports iterative reasoning processes

## ğŸ“Š Results

Our RCTS-RAG achieves significant improvements across multiple benchmarks.

![1756659766699](image/README/1756659766699.png)

*Results may vary based on model configurations and computational resources.*

## ğŸ¤ Citing RCTS

```
@misc{yang2025rerankingreasoningcontexttree,
      title={Re-ranking Reasoning Context with Tree Search Makes Large Vision-Language Models Stronger}, 
      author={Qi Yang and Chenghao Zhang and Lubin Fan and Kun Ding and Jieping Ye and Shiming Xiang},
      year={2025},
      eprint={2506.07785},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2506.07785}, 
}
```
